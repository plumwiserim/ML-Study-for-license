{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### help, dir \n",
    "\n",
    "만약 라이브러리명이 기억이 나지 않을 때 \\\n",
    "시험장에서 쓸 수 있는 방법 \n",
    "\n",
    "출처: https://www.youtube.com/watch?v=RRG44DJ6aCk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. help\n",
    "함수의 자세한 사용법을 알고싶을 때 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'function'>\n",
      "Help on function shapiro in module scipy.stats._morestats:\n",
      "\n",
      "shapiro(x)\n",
      "    Perform the Shapiro-Wilk test for normality.\n",
      "    \n",
      "    The Shapiro-Wilk test tests the null hypothesis that the\n",
      "    data was drawn from a normal distribution.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    x : array_like\n",
      "        Array of sample data.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    statistic : float\n",
      "        The test statistic.\n",
      "    p-value : float\n",
      "        The p-value for the hypothesis test.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    anderson : The Anderson-Darling test for normality\n",
      "    kstest : The Kolmogorov-Smirnov test for goodness of fit.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The algorithm used is described in [4]_ but censoring parameters as\n",
      "    described are not implemented. For N > 5000 the W test statistic is accurate\n",
      "    but the p-value may not be.\n",
      "    \n",
      "    The chance of rejecting the null hypothesis when it is true is close to 5%\n",
      "    regardless of sample size.\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] https://www.itl.nist.gov/div898/handbook/prc/section2/prc213.htm\n",
      "    .. [2] Shapiro, S. S. & Wilk, M.B (1965). An analysis of variance test for\n",
      "           normality (complete samples), Biometrika, Vol. 52, pp. 591-611.\n",
      "    .. [3] Razali, N. M. & Wah, Y. B. (2011) Power comparisons of Shapiro-Wilk,\n",
      "           Kolmogorov-Smirnov, Lilliefors and Anderson-Darling tests, Journal of\n",
      "           Statistical Modeling and Analytics, Vol. 2, pp. 21-33.\n",
      "    .. [4] ALGORITHM AS R94 APPL. STATIST. (1995) VOL. 44, NO. 4.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from scipy import stats\n",
      "    >>> rng = np.random.default_rng()\n",
      "    >>> x = stats.norm.rvs(loc=5, scale=3, size=100, random_state=rng)\n",
      "    >>> shapiro_test = stats.shapiro(x)\n",
      "    >>> shapiro_test\n",
      "    ShapiroResult(statistic=0.9813305735588074, pvalue=0.16855233907699585)\n",
      "    >>> shapiro_test.statistic\n",
      "    0.9813305735588074\n",
      "    >>> shapiro_test.pvalue\n",
      "    0.16855233907699585\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "print(type(shapiro))    # function \n",
    "print(help(shapiro))    # function의 사용법을 알고자할 때 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function shapiro in module scipy.stats._morestats:\n",
      "\n",
      "shapiro(x)\n",
      "    Perform the Shapiro-Wilk test for normality.\n",
      "    \n",
      "    The Shapiro-Wilk test tests the null hypothesis that the\n",
      "    data was drawn from a normal distribution.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    x : array_like\n",
      "        Array of sample data.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    statistic : float\n",
      "        The test statistic.\n",
      "    p-value : float\n",
      "        The p-value for the hypothesis test.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    anderson : The Anderson-Darling test for normality\n",
      "    kstest : The Kolmogorov-Smirnov test for goodness of fit.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The algorithm used is described in [4]_ but censoring parameters as\n",
      "    described are not implemented. For N > 5000 the W test statistic is accurate\n",
      "    but the p-value may not be.\n",
      "    \n",
      "    The chance of rejecting the null hypothesis when it is true is close to 5%\n",
      "    regardless of sample size.\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] https://www.itl.nist.gov/div898/handbook/prc/section2/prc213.htm\n",
      "    .. [2] Shapiro, S. S. & Wilk, M.B (1965). An analysis of variance test for\n",
      "           normality (complete samples), Biometrika, Vol. 52, pp. 591-611.\n",
      "    .. [3] Razali, N. M. & Wah, Y. B. (2011) Power comparisons of Shapiro-Wilk,\n",
      "           Kolmogorov-Smirnov, Lilliefors and Anderson-Darling tests, Journal of\n",
      "           Statistical Modeling and Analytics, Vol. 2, pp. 21-33.\n",
      "    .. [4] ALGORITHM AS R94 APPL. STATIST. (1995) VOL. 44, NO. 4.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from scipy import stats\n",
      "    >>> rng = np.random.default_rng()\n",
      "    >>> x = stats.norm.rvs(loc=5, scale=3, size=100, random_state=rng)\n",
      "    >>> shapiro_test = stats.shapiro(x)\n",
      "    >>> shapiro_test\n",
      "    ShapiroResult(statistic=0.9813305735588074, pvalue=0.16855233907699585)\n",
      "    >>> shapiro_test.statistic\n",
      "    0.9813305735588074\n",
      "    >>> shapiro_test.pvalue\n",
      "    0.16855233907699585\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import scipy            \n",
    "print(help(scipy.stats.shapiro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'module'>\n",
      "<class 'function'>\n",
      "Help on function get_dummies in module pandas.core.reshape.encoding:\n",
      "\n",
      "get_dummies(data, prefix=None, prefix_sep='_', dummy_na: 'bool' = False, columns=None, sparse: 'bool' = False, drop_first: 'bool' = False, dtype: 'Dtype | None' = None) -> 'DataFrame'\n",
      "    Convert categorical variable into dummy/indicator variables.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    data : array-like, Series, or DataFrame\n",
      "        Data of which to get dummy indicators.\n",
      "    prefix : str, list of str, or dict of str, default None\n",
      "        String to append DataFrame column names.\n",
      "        Pass a list with length equal to the number of columns\n",
      "        when calling get_dummies on a DataFrame. Alternatively, `prefix`\n",
      "        can be a dictionary mapping column names to prefixes.\n",
      "    prefix_sep : str, default '_'\n",
      "        If appending prefix, separator/delimiter to use. Or pass a\n",
      "        list or dictionary as with `prefix`.\n",
      "    dummy_na : bool, default False\n",
      "        Add a column to indicate NaNs, if False NaNs are ignored.\n",
      "    columns : list-like, default None\n",
      "        Column names in the DataFrame to be encoded.\n",
      "        If `columns` is None then all the columns with\n",
      "        `object`, `string`, or `category` dtype will be converted.\n",
      "    sparse : bool, default False\n",
      "        Whether the dummy-encoded columns should be backed by\n",
      "        a :class:`SparseArray` (True) or a regular NumPy array (False).\n",
      "    drop_first : bool, default False\n",
      "        Whether to get k-1 dummies out of k categorical levels by removing the\n",
      "        first level.\n",
      "    dtype : dtype, default np.uint8\n",
      "        Data type for new columns. Only a single dtype is allowed.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    DataFrame\n",
      "        Dummy-coded data.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    Series.str.get_dummies : Convert Series to dummy codes.\n",
      "    :func:`~pandas.from_dummies` : Convert dummy codes to categorical ``DataFrame``.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Reference :ref:`the user guide <reshaping.dummies>` for more examples.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> s = pd.Series(list('abca'))\n",
      "    \n",
      "    >>> pd.get_dummies(s)\n",
      "       a  b  c\n",
      "    0  1  0  0\n",
      "    1  0  1  0\n",
      "    2  0  0  1\n",
      "    3  1  0  0\n",
      "    \n",
      "    >>> s1 = ['a', 'b', np.nan]\n",
      "    \n",
      "    >>> pd.get_dummies(s1)\n",
      "       a  b\n",
      "    0  1  0\n",
      "    1  0  1\n",
      "    2  0  0\n",
      "    \n",
      "    >>> pd.get_dummies(s1, dummy_na=True)\n",
      "       a  b  NaN\n",
      "    0  1  0    0\n",
      "    1  0  1    0\n",
      "    2  0  0    1\n",
      "    \n",
      "    >>> df = pd.DataFrame({'A': ['a', 'b', 'a'], 'B': ['b', 'a', 'c'],\n",
      "    ...                    'C': [1, 2, 3]})\n",
      "    \n",
      "    >>> pd.get_dummies(df, prefix=['col1', 'col2'])\n",
      "       C  col1_a  col1_b  col2_a  col2_b  col2_c\n",
      "    0  1       1       0       0       1       0\n",
      "    1  2       0       1       1       0       0\n",
      "    2  3       1       0       0       0       1\n",
      "    \n",
      "    >>> pd.get_dummies(pd.Series(list('abcaa')))\n",
      "       a  b  c\n",
      "    0  1  0  0\n",
      "    1  0  1  0\n",
      "    2  0  0  1\n",
      "    3  1  0  0\n",
      "    4  1  0  0\n",
      "    \n",
      "    >>> pd.get_dummies(pd.Series(list('abcaa')), drop_first=True)\n",
      "       b  c\n",
      "    0  0  0\n",
      "    1  1  0\n",
      "    2  0  1\n",
      "    3  0  0\n",
      "    4  0  0\n",
      "    \n",
      "    >>> pd.get_dummies(pd.Series(list('abc')), dtype=float)\n",
      "         a    b    c\n",
      "    0  1.0  0.0  0.0\n",
      "    1  0.0  1.0  0.0\n",
      "    2  0.0  0.0  1.0\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(type(pd)) # module\n",
    "print(type(pd.get_dummies)) # function \n",
    "print(help(pd.get_dummies))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. dir\n",
    "라이브러리에 어떤 함수가 포함되었느지 알고 싶을 때 \\\n",
    "(대신 내가 원하는 것이 어떤 함수인지는 정확하게 알기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ConstantInputWarning', 'DegenerateDataWarning', 'FitError', 'NearConstantInputWarning', 'NumericalInverseHermite', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_axis_nan_policy', '_biasedurn', '_binned_statistic', '_binomtest', '_boost', '_common', '_constants', '_continuous_distns', '_crosstab', '_discrete_distns', '_distn_infrastructure', '_distr_params', '_entropy', '_fit', '_hypotests', '_hypotests_pythran', '_kde', '_ksstats', '_levy_stable', '_mannwhitneyu', '_morestats', '_mstats_basic', '_mstats_extras', '_multivariate', '_mvn', '_page_trend_test', '_qmc', '_qmc_cy', '_relative_risk', '_resampling', '_rvs_sampling', '_sobol', '_statlib', '_stats', '_stats_mstats_common', '_stats_py', '_tukeylambda_stats', '_unuran', '_variation', '_warnings_errors', 'alexandergovern', 'alpha', 'anderson', 'anderson_ksamp', 'anglit', 'ansari', 'arcsine', 'argus', 'barnard_exact', 'bartlett', 'bayes_mvs', 'bernoulli', 'beta', 'betabinom', 'betaprime', 'biasedurn', 'binned_statistic', 'binned_statistic_2d', 'binned_statistic_dd', 'binom', 'binom_test', 'binomtest', 'boltzmann', 'bootstrap', 'boschloo_exact', 'boxcox', 'boxcox_llf', 'boxcox_normmax', 'boxcox_normplot', 'bradford', 'brunnermunzel', 'burr', 'burr12', 'cauchy', 'chi', 'chi2', 'chi2_contingency', 'chisquare', 'circmean', 'circstd', 'circvar', 'combine_pvalues', 'contingency', 'cosine', 'cramervonmises', 'cramervonmises_2samp', 'crystalball', 'cumfreq', 'describe', 'dgamma', 'differential_entropy', 'dirichlet', 'distributions', 'dlaplace', 'dweibull', 'energy_distance', 'entropy', 'epps_singleton_2samp', 'erlang', 'expon', 'exponnorm', 'exponpow', 'exponweib', 'f', 'f_oneway', 'fatiguelife', 'find_repeats', 'fisher_exact', 'fisk', 'fit', 'fligner', 'foldcauchy', 'foldnorm', 'friedmanchisquare', 'gamma', 'gausshyper', 'gaussian_kde', 'genexpon', 'genextreme', 'gengamma', 'genhalflogistic', 'genhyperbolic', 'geninvgauss', 'genlogistic', 'gennorm', 'genpareto', 'geom', 'gibrat', 'gilbrat', 'gmean', 'gompertz', 'gstd', 'gumbel_l', 'gumbel_r', 'gzscore', 'halfcauchy', 'halfgennorm', 'halflogistic', 'halfnorm', 'hmean', 'hypergeom', 'hypsecant', 'invgamma', 'invgauss', 'invweibull', 'invwishart', 'iqr', 'jarque_bera', 'johnsonsb', 'johnsonsu', 'kappa3', 'kappa4', 'kde', 'kendalltau', 'kruskal', 'ks_1samp', 'ks_2samp', 'ksone', 'kstat', 'kstatvar', 'kstest', 'kstwo', 'kstwobign', 'kurtosis', 'kurtosistest', 'laplace', 'laplace_asymmetric', 'levene', 'levy', 'levy_l', 'levy_stable', 'linregress', 'loggamma', 'logistic', 'loglaplace', 'lognorm', 'logser', 'loguniform', 'lomax', 'mannwhitneyu', 'matrix_normal', 'maxwell', 'median_abs_deviation', 'median_test', 'mielke', 'mode', 'moment', 'monte_carlo_test', 'mood', 'morestats', 'moyal', 'mstats', 'mstats_basic', 'mstats_extras', 'multinomial', 'multiscale_graphcorr', 'multivariate_hypergeom', 'multivariate_normal', 'multivariate_t', 'mvn', 'mvsdist', 'nakagami', 'nbinom', 'ncf', 'nchypergeom_fisher', 'nchypergeom_wallenius', 'nct', 'ncx2', 'nhypergeom', 'norm', 'normaltest', 'norminvgauss', 'obrientransform', 'ortho_group', 'page_trend_test', 'pareto', 'pearson3', 'pearsonr', 'percentileofscore', 'permutation_test', 'planck', 'pmean', 'pointbiserialr', 'poisson', 'power_divergence', 'powerlaw', 'powerlognorm', 'powernorm', 'ppcc_max', 'ppcc_plot', 'probplot', 'qmc', 'randint', 'random_correlation', 'rankdata', 'ranksums', 'rayleigh', 'rdist', 'recipinvgauss', 'reciprocal', 'relfreq', 'rice', 'rv_continuous', 'rv_discrete', 'rv_histogram', 'rvs_ratio_uniforms', 'scoreatpercentile', 'sem', 'semicircular', 'shapiro', 'siegelslopes', 'sigmaclip', 'skellam', 'skew', 'skewcauchy', 'skewnorm', 'skewtest', 'somersd', 'spearmanr', 'special_ortho_group', 'statlib', 'stats', 'studentized_range', 't', 'test', 'theilslopes', 'tiecorrect', 'tmax', 'tmean', 'tmin', 'trapezoid', 'trapz', 'triang', 'trim1', 'trim_mean', 'trimboth', 'truncexpon', 'truncnorm', 'truncweibull_min', 'tsem', 'tstd', 'ttest_1samp', 'ttest_ind', 'ttest_ind_from_stats', 'ttest_rel', 'tukey_hsd', 'tukeylambda', 'tvar', 'uniform', 'unitary_group', 'variation', 'vonmises', 'vonmises_line', 'wald', 'wasserstein_distance', 'weibull_max', 'weibull_min', 'weightedtau', 'wilcoxon', 'wishart', 'wrapcauchy', 'yeojohnson', 'yeojohnson_llf', 'yeojohnson_normmax', 'yeojohnson_normplot', 'yulesimon', 'zipf', 'zipfian', 'zmap', 'zscore']\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "print(dir(scipy.stats)) # scipy.stats 에 포함된 함수 목록 확인(list --> ndarray, Series, DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ConstantInputWarning' 'DegenerateDataWarning' 'FitError'\n",
      " 'NearConstantInputWarning' 'NumericalInverseHermite' '__all__'\n",
      " '__builtins__' '__cached__' '__doc__' '__file__' '__loader__' '__name__'\n",
      " '__package__' '__path__' '__spec__' '_axis_nan_policy' '_biasedurn'\n",
      " '_binned_statistic' '_binomtest' '_boost' '_common' '_constants'\n",
      " '_continuous_distns' '_crosstab' '_discrete_distns'\n",
      " '_distn_infrastructure' '_distr_params' '_entropy' '_fit' '_hypotests'\n",
      " '_hypotests_pythran' '_kde' '_ksstats' '_levy_stable' '_mannwhitneyu'\n",
      " '_morestats' '_mstats_basic' '_mstats_extras' '_multivariate' '_mvn'\n",
      " '_page_trend_test' '_qmc' '_qmc_cy' '_relative_risk' '_resampling'\n",
      " '_rvs_sampling' '_sobol' '_statlib' '_stats' '_stats_mstats_common'\n",
      " '_stats_py' '_tukeylambda_stats' '_unuran' '_variation'\n",
      " '_warnings_errors' 'alexandergovern' 'alpha' 'anderson' 'anderson_ksamp'\n",
      " 'anglit' 'ansari' 'arcsine' 'argus' 'barnard_exact' 'bartlett'\n",
      " 'bayes_mvs' 'bernoulli' 'beta' 'betabinom' 'betaprime' 'biasedurn'\n",
      " 'binned_statistic' 'binned_statistic_2d' 'binned_statistic_dd' 'binom'\n",
      " 'binom_test' 'binomtest' 'boltzmann' 'bootstrap' 'boschloo_exact'\n",
      " 'boxcox' 'boxcox_llf' 'boxcox_normmax' 'boxcox_normplot' 'bradford'\n",
      " 'brunnermunzel' 'burr' 'burr12' 'cauchy' 'chi' 'chi2' 'chi2_contingency'\n",
      " 'chisquare' 'circmean' 'circstd' 'circvar' 'combine_pvalues'\n",
      " 'contingency' 'cosine' 'cramervonmises' 'cramervonmises_2samp'\n",
      " 'crystalball' 'cumfreq' 'describe' 'dgamma' 'differential_entropy'\n",
      " 'dirichlet' 'distributions' 'dlaplace' 'dweibull' 'energy_distance'\n",
      " 'entropy' 'epps_singleton_2samp' 'erlang' 'expon' 'exponnorm' 'exponpow'\n",
      " 'exponweib' 'f' 'f_oneway' 'fatiguelife' 'find_repeats' 'fisher_exact'\n",
      " 'fisk' 'fit' 'fligner' 'foldcauchy' 'foldnorm' 'friedmanchisquare'\n",
      " 'gamma' 'gausshyper' 'gaussian_kde' 'genexpon' 'genextreme' 'gengamma'\n",
      " 'genhalflogistic' 'genhyperbolic' 'geninvgauss' 'genlogistic' 'gennorm'\n",
      " 'genpareto' 'geom' 'gibrat' 'gilbrat' 'gmean' 'gompertz' 'gstd'\n",
      " 'gumbel_l' 'gumbel_r' 'gzscore' 'halfcauchy' 'halfgennorm' 'halflogistic'\n",
      " 'halfnorm' 'hmean' 'hypergeom' 'hypsecant' 'invgamma' 'invgauss'\n",
      " 'invweibull' 'invwishart' 'iqr' 'jarque_bera' 'johnsonsb' 'johnsonsu'\n",
      " 'kappa3' 'kappa4' 'kde' 'kendalltau' 'kruskal' 'ks_1samp' 'ks_2samp'\n",
      " 'ksone' 'kstat' 'kstatvar' 'kstest' 'kstwo' 'kstwobign' 'kurtosis'\n",
      " 'kurtosistest' 'laplace' 'laplace_asymmetric' 'levene' 'levy' 'levy_l'\n",
      " 'levy_stable' 'linregress' 'loggamma' 'logistic' 'loglaplace' 'lognorm'\n",
      " 'logser' 'loguniform' 'lomax' 'mannwhitneyu' 'matrix_normal' 'maxwell'\n",
      " 'median_abs_deviation' 'median_test' 'mielke' 'mode' 'moment'\n",
      " 'monte_carlo_test' 'mood' 'morestats' 'moyal' 'mstats' 'mstats_basic'\n",
      " 'mstats_extras' 'multinomial' 'multiscale_graphcorr'\n",
      " 'multivariate_hypergeom' 'multivariate_normal' 'multivariate_t' 'mvn'\n",
      " 'mvsdist' 'nakagami' 'nbinom' 'ncf' 'nchypergeom_fisher'\n",
      " 'nchypergeom_wallenius' 'nct' 'ncx2' 'nhypergeom' 'norm' 'normaltest'\n",
      " 'norminvgauss' 'obrientransform' 'ortho_group' 'page_trend_test' 'pareto'\n",
      " 'pearson3' 'pearsonr' 'percentileofscore' 'permutation_test' 'planck'\n",
      " 'pmean' 'pointbiserialr' 'poisson' 'power_divergence' 'powerlaw'\n",
      " 'powerlognorm' 'powernorm' 'ppcc_max' 'ppcc_plot' 'probplot' 'qmc'\n",
      " 'randint' 'random_correlation' 'rankdata' 'ranksums' 'rayleigh' 'rdist'\n",
      " 'recipinvgauss' 'reciprocal' 'relfreq' 'rice' 'rv_continuous'\n",
      " 'rv_discrete' 'rv_histogram' 'rvs_ratio_uniforms' 'scoreatpercentile'\n",
      " 'sem' 'semicircular' 'shapiro' 'siegelslopes' 'sigmaclip' 'skellam'\n",
      " 'skew' 'skewcauchy' 'skewnorm' 'skewtest' 'somersd' 'spearmanr'\n",
      " 'special_ortho_group' 'statlib' 'stats' 'studentized_range' 't' 'test'\n",
      " 'theilslopes' 'tiecorrect' 'tmax' 'tmean' 'tmin' 'trapezoid' 'trapz'\n",
      " 'triang' 'trim1' 'trim_mean' 'trimboth' 'truncexpon' 'truncnorm'\n",
      " 'truncweibull_min' 'tsem' 'tstd' 'ttest_1samp' 'ttest_ind'\n",
      " 'ttest_ind_from_stats' 'ttest_rel' 'tukey_hsd' 'tukeylambda' 'tvar'\n",
      " 'uniform' 'unitary_group' 'variation' 'vonmises' 'vonmises_line' 'wald'\n",
      " 'wasserstein_distance' 'weibull_max' 'weibull_min' 'weightedtau'\n",
      " 'wilcoxon' 'wishart' 'wrapcauchy' 'yeojohnson' 'yeojohnson_llf'\n",
      " 'yeojohnson_normmax' 'yeojohnson_normplot' 'yulesimon' 'zipf' 'zipfian'\n",
      " 'zmap' 'zscore']\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import numpy as np \n",
    "\n",
    "print(np.array(dir(scipy.stats)))   # scipy.stats 에 포함된 함수 목록 확인(ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89                   chi\n",
      "90                  chi2\n",
      "91      chi2_contingency\n",
      "92             chisquare\n",
      "128    friedmanchisquare\n",
      "dtype: object\n",
      "296             ttest_1samp\n",
      "297               ttest_ind\n",
      "298    ttest_ind_from_stats\n",
      "299               ttest_rel\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import pandas as pd \n",
    "\n",
    "s = pd.Series(dir(scipy.stats)) # scipy.stats 에 포함된 함수 목록 확인(Series)\n",
    "\n",
    "# print(s)\n",
    "# print(s.str.contains('chi'))\n",
    "print(s[s.str.contains('chi')])\n",
    "print(s[s.str.contains('ttest')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConstantInputWarning\n",
      "DegenerateDataWarning\n",
      "FitError\n",
      "NearConstantInputWarning\n",
      "NumericalInverseHermite\n",
      "__all__\n",
      "__builtins__\n",
      "__cached__\n",
      "__doc__\n",
      "__file__\n",
      "__loader__\n",
      "__name__\n",
      "__package__\n",
      "__path__\n",
      "__spec__\n",
      "_axis_nan_policy\n",
      "_biasedurn\n",
      "_binned_statistic\n",
      "_binomtest\n",
      "_boost\n",
      "_common\n",
      "_constants\n",
      "_continuous_distns\n",
      "_crosstab\n",
      "_discrete_distns\n",
      "_distn_infrastructure\n",
      "_distr_params\n",
      "_entropy\n",
      "_fit\n",
      "_hypotests\n",
      "_hypotests_pythran\n",
      "_kde\n",
      "_ksstats\n",
      "_levy_stable\n",
      "_mannwhitneyu\n",
      "_morestats\n",
      "_mstats_basic\n",
      "_mstats_extras\n",
      "_multivariate\n",
      "_mvn\n",
      "_page_trend_test\n",
      "_qmc\n",
      "_qmc_cy\n",
      "_relative_risk\n",
      "_resampling\n",
      "_rvs_sampling\n",
      "_sobol\n",
      "_statlib\n",
      "_stats\n",
      "_stats_mstats_common\n",
      "_stats_py\n",
      "_tukeylambda_stats\n",
      "_unuran\n",
      "_variation\n",
      "_warnings_errors\n",
      "alexandergovern\n",
      "alpha\n",
      "anderson\n",
      "anderson_ksamp\n",
      "anglit\n",
      "ansari\n",
      "arcsine\n",
      "argus\n",
      "barnard_exact\n",
      "bartlett\n",
      "bayes_mvs\n",
      "bernoulli\n",
      "beta\n",
      "betabinom\n",
      "betaprime\n",
      "biasedurn\n",
      "binned_statistic\n",
      "binned_statistic_2d\n",
      "binned_statistic_dd\n",
      "binom\n",
      "binom_test\n",
      "binomtest\n",
      "boltzmann\n",
      "bootstrap\n",
      "boschloo_exact\n",
      "boxcox\n",
      "boxcox_llf\n",
      "boxcox_normmax\n",
      "boxcox_normplot\n",
      "bradford\n",
      "brunnermunzel\n",
      "burr\n",
      "burr12\n",
      "cauchy\n",
      "chi\n",
      "chi2\n",
      "chi2_contingency\n",
      "chisquare\n",
      "circmean\n",
      "circstd\n",
      "circvar\n",
      "combine_pvalues\n",
      "contingency\n",
      "cosine\n",
      "cramervonmises\n",
      "cramervonmises_2samp\n",
      "crystalball\n",
      "cumfreq\n",
      "describe\n",
      "dgamma\n",
      "differential_entropy\n",
      "dirichlet\n",
      "distributions\n",
      "dlaplace\n",
      "dweibull\n",
      "energy_distance\n",
      "entropy\n",
      "epps_singleton_2samp\n",
      "erlang\n",
      "expon\n",
      "exponnorm\n",
      "exponpow\n",
      "exponweib\n",
      "f\n",
      "f_oneway\n",
      "fatiguelife\n",
      "find_repeats\n",
      "fisher_exact\n",
      "fisk\n",
      "fit\n",
      "fligner\n",
      "foldcauchy\n",
      "foldnorm\n",
      "friedmanchisquare\n",
      "gamma\n",
      "gausshyper\n",
      "gaussian_kde\n",
      "genexpon\n",
      "genextreme\n",
      "gengamma\n",
      "genhalflogistic\n",
      "genhyperbolic\n",
      "geninvgauss\n",
      "genlogistic\n",
      "gennorm\n",
      "genpareto\n",
      "geom\n",
      "gibrat\n",
      "gilbrat\n",
      "gmean\n",
      "gompertz\n",
      "gstd\n",
      "gumbel_l\n",
      "gumbel_r\n",
      "gzscore\n",
      "halfcauchy\n",
      "halfgennorm\n",
      "halflogistic\n",
      "halfnorm\n",
      "hmean\n",
      "hypergeom\n",
      "hypsecant\n",
      "invgamma\n",
      "invgauss\n",
      "invweibull\n",
      "invwishart\n",
      "iqr\n",
      "jarque_bera\n",
      "johnsonsb\n",
      "johnsonsu\n",
      "kappa3\n",
      "kappa4\n",
      "kde\n",
      "kendalltau\n",
      "kruskal\n",
      "ks_1samp\n",
      "ks_2samp\n",
      "ksone\n",
      "kstat\n",
      "kstatvar\n",
      "kstest\n",
      "kstwo\n",
      "kstwobign\n",
      "kurtosis\n",
      "kurtosistest\n",
      "laplace\n",
      "laplace_asymmetric\n",
      "levene\n",
      "levy\n",
      "levy_l\n",
      "levy_stable\n",
      "linregress\n",
      "loggamma\n",
      "logistic\n",
      "loglaplace\n",
      "lognorm\n",
      "logser\n",
      "loguniform\n",
      "lomax\n",
      "mannwhitneyu\n",
      "matrix_normal\n",
      "maxwell\n",
      "median_abs_deviation\n",
      "median_test\n",
      "mielke\n",
      "mode\n",
      "moment\n",
      "monte_carlo_test\n",
      "mood\n",
      "morestats\n",
      "moyal\n",
      "mstats\n",
      "mstats_basic\n",
      "mstats_extras\n",
      "multinomial\n",
      "multiscale_graphcorr\n",
      "multivariate_hypergeom\n",
      "multivariate_normal\n",
      "multivariate_t\n",
      "mvn\n",
      "mvsdist\n",
      "nakagami\n",
      "nbinom\n",
      "ncf\n",
      "nchypergeom_fisher\n",
      "nchypergeom_wallenius\n",
      "nct\n",
      "ncx2\n",
      "nhypergeom\n",
      "norm\n",
      "normaltest\n",
      "norminvgauss\n",
      "obrientransform\n",
      "ortho_group\n",
      "page_trend_test\n",
      "pareto\n",
      "pearson3\n",
      "pearsonr\n",
      "percentileofscore\n",
      "permutation_test\n",
      "planck\n",
      "pmean\n",
      "pointbiserialr\n",
      "poisson\n",
      "power_divergence\n",
      "powerlaw\n",
      "powerlognorm\n",
      "powernorm\n",
      "ppcc_max\n",
      "ppcc_plot\n",
      "probplot\n",
      "qmc\n",
      "randint\n",
      "random_correlation\n",
      "rankdata\n",
      "ranksums\n",
      "rayleigh\n",
      "rdist\n",
      "recipinvgauss\n",
      "reciprocal\n",
      "relfreq\n",
      "rice\n",
      "rv_continuous\n",
      "rv_discrete\n",
      "rv_histogram\n",
      "rvs_ratio_uniforms\n",
      "scoreatpercentile\n",
      "sem\n",
      "semicircular\n",
      "shapiro\n",
      "siegelslopes\n",
      "sigmaclip\n",
      "skellam\n",
      "skew\n",
      "skewcauchy\n",
      "skewnorm\n",
      "skewtest\n",
      "somersd\n",
      "spearmanr\n",
      "special_ortho_group\n",
      "statlib\n",
      "stats\n",
      "studentized_range\n",
      "t\n",
      "test\n",
      "theilslopes\n",
      "tiecorrect\n",
      "tmax\n",
      "tmean\n",
      "tmin\n",
      "trapezoid\n",
      "trapz\n",
      "triang\n",
      "trim1\n",
      "trim_mean\n",
      "trimboth\n",
      "truncexpon\n",
      "truncnorm\n",
      "truncweibull_min\n",
      "tsem\n",
      "tstd\n",
      "ttest_1samp\n",
      "ttest_ind\n",
      "ttest_ind_from_stats\n",
      "ttest_rel\n",
      "tukey_hsd\n",
      "tukeylambda\n",
      "tvar\n",
      "uniform\n",
      "unitary_group\n",
      "variation\n",
      "vonmises\n",
      "vonmises_line\n",
      "wald\n",
      "wasserstein_distance\n",
      "weibull_max\n",
      "weibull_min\n",
      "weightedtau\n",
      "wilcoxon\n",
      "wishart\n",
      "wrapcauchy\n",
      "yeojohnson\n",
      "yeojohnson_llf\n",
      "yeojohnson_normmax\n",
      "yeojohnson_normplot\n",
      "yulesimon\n",
      "zipf\n",
      "zipfian\n",
      "zmap\n",
      "zscore\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "library_list = dir(scipy.stats)\n",
    "\n",
    "for i in library_list: \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['calibration', 'cluster', 'covariance', 'cross_decomposition', 'datasets', 'decomposition', 'dummy', 'ensemble', 'exceptions', 'experimental', 'externals', 'feature_extraction', 'feature_selection', 'gaussian_process', 'inspection', 'isotonic', 'kernel_approximation', 'kernel_ridge', 'linear_model', 'manifold', 'metrics', 'mixture', 'model_selection', 'multiclass', 'multioutput', 'naive_bayes', 'neighbors', 'neural_network', 'pipeline', 'preprocessing', 'random_projection', 'semi_supervised', 'svm', 'tree', 'discriminant_analysis', 'impute', 'compose', 'clone', 'get_config', 'set_config', 'config_context', 'show_versions']\n"
     ]
    }
   ],
   "source": [
    "# sklearn \n",
    "import sklearn \n",
    "\n",
    "# sklearn의 패키지 \n",
    "print(sklearn.__all__)\n",
    "\n",
    "# tree\n",
    "# --> DecisionTreeClassifier\n",
    "# --> DecisionTreeRegressor \n",
    "# : 학습 시에 이상치까지 포함하기 때문에 과적합의 문제가 있다 \n",
    "# 하지만 변수의 중요도까지 도출해내기도 한다. \n",
    "# 그리고 앙상블 전에 해당 모델을 활용하여 적정 Depth을 알아낼 수 있다. \n",
    "# model.get_depth() \n",
    "\n",
    "# ensemble \n",
    "# --> RandomForestRegressor \n",
    "# --> GradientBoostingRegressor \n",
    "\n",
    "# linear_model \n",
    "# --> LinearRegression \n",
    "# --> LogisticRegression \n",
    "# 성능 비교에 활용 \n",
    "\n",
    "# neighbors\n",
    "# --> KNeighborsClassifier \n",
    "\n",
    "# metrics\n",
    "# --> dir 활용 \n",
    "# model_selection \n",
    "# --> train_test_split \n",
    "# preprocessing \n",
    "# --> MinMaxScaler\n",
    "# --> StandardScaler\n",
    "# --> LabelEncoder \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseDecisionTree\n",
      "DecisionTreeClassifier\n",
      "DecisionTreeRegressor\n",
      "ExtraTreeClassifier\n",
      "ExtraTreeRegressor\n",
      "__all__\n",
      "__builtins__\n",
      "__cached__\n",
      "__doc__\n",
      "__file__\n",
      "__loader__\n",
      "__name__\n",
      "__package__\n",
      "__path__\n",
      "__spec__\n",
      "_classes\n",
      "_criterion\n",
      "_export\n",
      "_reingold_tilford\n",
      "_splitter\n",
      "_tree\n",
      "_utils\n",
      "export_graphviz\n",
      "export_text\n",
      "plot_tree\n"
     ]
    }
   ],
   "source": [
    "import sklearn.tree\n",
    "\n",
    "tree_list = dir(sklearn.tree)\n",
    "for i in tree_list: \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier\n",
      "AdaBoostRegressor\n",
      "BaggingClassifier\n",
      "BaggingRegressor\n",
      "BaseEnsemble\n",
      "ExtraTreesClassifier\n",
      "ExtraTreesRegressor\n",
      "GradientBoostingClassifier\n",
      "GradientBoostingRegressor\n",
      "HistGradientBoostingClassifier\n",
      "HistGradientBoostingRegressor\n",
      "IsolationForest\n",
      "RandomForestClassifier\n",
      "RandomForestRegressor\n",
      "RandomTreesEmbedding\n",
      "StackingClassifier\n",
      "StackingRegressor\n",
      "VotingClassifier\n",
      "VotingRegressor\n",
      "__all__\n",
      "__builtins__\n",
      "__cached__\n",
      "__doc__\n",
      "__file__\n",
      "__loader__\n",
      "__name__\n",
      "__package__\n",
      "__path__\n",
      "__spec__\n",
      "_bagging\n",
      "_base\n",
      "_forest\n",
      "_gb\n",
      "_gb_losses\n",
      "_gradient_boosting\n",
      "_hist_gradient_boosting\n",
      "_iforest\n",
      "_stacking\n",
      "_voting\n",
      "_weight_boosting\n"
     ]
    }
   ],
   "source": [
    "import sklearn.ensemble\n",
    "\n",
    "tree_list = dir(sklearn.ensemble)\n",
    "for i in tree_list: \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                 AdaBoostClassifier\n",
      "2                  BaggingClassifier\n",
      "5               ExtraTreesClassifier\n",
      "7         GradientBoostingClassifier\n",
      "9     HistGradientBoostingClassifier\n",
      "12            RandomForestClassifier\n",
      "15                StackingClassifier\n",
      "17                  VotingClassifier\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import sklearn.ensemble \n",
    "import pandas as pd \n",
    "\n",
    "s = pd.Series(dir(sklearn.ensemble))\n",
    "print(s[s.str.contains('Classifier')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RandomForestClassifier in module sklearn.ensemble._forest:\n",
      "\n",
      "class RandomForestClassifier(ForestClassifier)\n",
      " |  RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
      " |  \n",
      " |  A random forest classifier.\n",
      " |  \n",
      " |  A random forest is a meta estimator that fits a number of decision tree\n",
      " |  classifiers on various sub-samples of the dataset and uses averaging to\n",
      " |  improve the predictive accuracy and control over-fitting.\n",
      " |  The sub-sample size is controlled with the `max_samples` parameter if\n",
      " |  `bootstrap=True` (default), otherwise the whole dataset is used to build\n",
      " |  each tree.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <forest>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_estimators : int, default=100\n",
      " |      The number of trees in the forest.\n",
      " |  \n",
      " |      .. versionchanged:: 0.22\n",
      " |         The default value of ``n_estimators`` changed from 10 to 100\n",
      " |         in 0.22.\n",
      " |  \n",
      " |  criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n",
      " |      The function to measure the quality of a split. Supported criteria are\n",
      " |      \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n",
      " |      Shannon information gain, see :ref:`tree_mathematical_formulation`.\n",
      " |      Note: This parameter is tree-specific.\n",
      " |  \n",
      " |  max_depth : int, default=None\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `max(1, int(max_features * n_features_in_))` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      .. versionchanged:: 1.1\n",
      " |          The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\n",
      " |  \n",
      " |      .. deprecated:: 1.1\n",
      " |          The `\"auto\"` option was deprecated in 1.1 and will be removed\n",
      " |          in 1.3.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  bootstrap : bool, default=True\n",
      " |      Whether bootstrap samples are used when building trees. If False, the\n",
      " |      whole dataset is used to build each tree.\n",
      " |  \n",
      " |  oob_score : bool, default=False\n",
      " |      Whether to use out-of-bag samples to estimate the generalization score.\n",
      " |      Only available if bootstrap=True.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
      " |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
      " |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      " |      context. ``-1`` means using all processors. See :term:`Glossary\n",
      " |      <n_jobs>` for more details.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls both the randomness of the bootstrapping of the samples used\n",
      " |      when building trees (if ``bootstrap=True``) and the sampling of the\n",
      " |      features to consider when looking for the best split at each node\n",
      " |      (if ``max_features < n_features``).\n",
      " |      See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Controls the verbosity when fitting and predicting.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      " |      new forest. See :term:`Glossary <warm_start>` and\n",
      " |      :ref:`gradient_boosting_warm_start` for details.\n",
      " |  \n",
      " |  class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one. For\n",
      " |      multi-output problems, a list of dicts can be provided in the same\n",
      " |      order as the columns of y.\n",
      " |  \n",
      " |      Note that for multioutput (including multilabel) weights should be\n",
      " |      defined for each class of every column in its own dict. For example,\n",
      " |      for four-class multilabel classification weights should be\n",
      " |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      " |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |      The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
      " |      weights are computed based on the bootstrap sample for every tree\n",
      " |      grown.\n",
      " |  \n",
      " |      For multi-output, the weights of each column of y will be multiplied.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  max_samples : int or float, default=None\n",
      " |      If bootstrap is True, the number of samples to draw from X\n",
      " |      to train each base estimator.\n",
      " |  \n",
      " |      - If None (default), then draw `X.shape[0]` samples.\n",
      " |      - If int, then draw `max_samples` samples.\n",
      " |      - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
      " |        `max_samples` should be in the interval `(0.0, 1.0]`.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  estimator_ : :class:`~sklearn.tree.DecisionTreeClassifier`\n",
      " |      The child estimator template used to create the collection of fitted\n",
      " |      sub-estimators.\n",
      " |  \n",
      " |      .. versionadded:: 1.2\n",
      " |         `base_estimator_` was renamed to `estimator_`.\n",
      " |  \n",
      " |  base_estimator_ : DecisionTreeClassifier\n",
      " |      The child estimator template used to create the collection of fitted\n",
      " |      sub-estimators.\n",
      " |  \n",
      " |      .. deprecated:: 1.2\n",
      " |          `base_estimator_` is deprecated and will be removed in 1.4.\n",
      " |          Use `estimator_` instead.\n",
      " |  \n",
      " |  estimators_ : list of DecisionTreeClassifier\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,) or a list of such arrays\n",
      " |      The classes labels (single output problem), or a list of arrays of\n",
      " |      class labels (multi-output problem).\n",
      " |  \n",
      " |  n_classes_ : int or list\n",
      " |      The number of classes (single output problem), or a list containing the\n",
      " |      number of classes for each output (multi-output problem).\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  oob_score_ : float\n",
      " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      " |      This attribute exists only when ``oob_score`` is True.\n",
      " |  \n",
      " |  oob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)\n",
      " |      Decision function computed with out-of-bag estimate on the training\n",
      " |      set. If n_estimators is small it might be possible that a data point\n",
      " |      was never left out during the bootstrap. In this case,\n",
      " |      `oob_decision_function_` might contain NaN. This attribute exists\n",
      " |      only when ``oob_score`` is True.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n",
      " |  sklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized\n",
      " |      tree classifiers.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data,\n",
      " |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      " |  of the criterion is identical for several splits enumerated during the\n",
      " |  search of the best split. To obtain a deterministic behaviour during\n",
      " |  fitting, ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import RandomForestClassifier\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n",
      " |  ...                            n_informative=2, n_redundant=0,\n",
      " |  ...                            random_state=0, shuffle=False)\n",
      " |  >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
      " |  >>> clf.fit(X, y)\n",
      " |  RandomForestClassifier(...)\n",
      " |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomForestClassifier\n",
      " |      ForestClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseForest\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ForestClassifier:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class for X.\n",
      " |      \n",
      " |      The predicted class of an input sample is a vote by the trees in\n",
      " |      the forest, weighted by their probability estimates. That is,\n",
      " |      the predicted class is the one with highest mean probability\n",
      " |      estimate across the trees.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      The predicted class log-probabilities of an input sample is computed as\n",
      " |      the log of the mean predicted class probabilities of the trees in the\n",
      " |      forest.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      The predicted class probabilities of an input sample are computed as\n",
      " |      the mean predicted class probabilities of the trees in the forest.\n",
      " |      The class probability of a single tree is the fraction of samples of\n",
      " |      the same class in a leaf.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseForest:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the forest to X, return leaf indices.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : ndarray of shape (n_samples, n_estimators)\n",
      " |          For each datapoint x in X and for each tree in the forest,\n",
      " |          return the index of the leaf x ends up in.\n",
      " |  \n",
      " |  decision_path(self, X)\n",
      " |      Return the decision path in the forest.\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      " |          Return a node indicator matrix where non zero elements indicates\n",
      " |          that the samples goes through the nodes. The matrix is of CSR\n",
      " |          format.\n",
      " |      \n",
      " |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n",
      " |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      " |          gives the indicator value for the i-th estimator.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a forest of trees from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, its dtype will be converted\n",
      " |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseForest:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      The impurity-based feature importances.\n",
      " |      \n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          The values of this array sum to 1, unless all trees are single node\n",
      " |          trees consisting of only the root node, in which case it will be an\n",
      " |          array of zeros.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  base_estimator_\n",
      " |      Estimator used to grow the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "help(RandomForestClassifier)\n",
    "# 파라미터 중 * 앞에 위치한 것들을 자주 사용 \n",
    "# 그 뒤로 위치한 것들은 앞에 위치한 순서대로 자주 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54                   apply\n",
      "55         base_estimator_\n",
      "56           decision_path\n",
      "57    feature_importances_\n",
      "58                     fit\n",
      "59              get_params\n",
      "60                 predict\n",
      "61       predict_log_proba\n",
      "62           predict_proba\n",
      "63                   score\n",
      "64              set_params\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "s = pd.Series(dir(RandomForestClassifier))\n",
    "s = s[s.str[0] != '_']\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function value_counts in module pandas.core.base:\n",
      "\n",
      "value_counts(self, normalize: 'bool' = False, sort: 'bool' = True, ascending: 'bool' = False, bins=None, dropna: 'bool' = True) -> 'Series'\n",
      "    Return a Series containing counts of unique values.\n",
      "    \n",
      "    The resulting object will be in descending order so that the\n",
      "    first element is the most frequently-occurring element.\n",
      "    Excludes NA values by default.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    normalize : bool, default False\n",
      "        If True then the object returned will contain the relative\n",
      "        frequencies of the unique values.\n",
      "    sort : bool, default True\n",
      "        Sort by frequencies.\n",
      "    ascending : bool, default False\n",
      "        Sort in ascending order.\n",
      "    bins : int, optional\n",
      "        Rather than count values, group them into half-open bins,\n",
      "        a convenience for ``pd.cut``, only works with numeric data.\n",
      "    dropna : bool, default True\n",
      "        Don't include counts of NaN.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    Series\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    Series.count: Number of non-NA elements in a Series.\n",
      "    DataFrame.count: Number of non-NA elements in a DataFrame.\n",
      "    DataFrame.value_counts: Equivalent method on DataFrames.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> index = pd.Index([3, 1, 2, 3, 4, np.nan])\n",
      "    >>> index.value_counts()\n",
      "    3.0    2\n",
      "    1.0    1\n",
      "    2.0    1\n",
      "    4.0    1\n",
      "    dtype: int64\n",
      "    \n",
      "    With `normalize` set to `True`, returns the relative frequency by\n",
      "    dividing all values by the sum of values.\n",
      "    \n",
      "    >>> s = pd.Series([3, 1, 2, 3, 4, np.nan])\n",
      "    >>> s.value_counts(normalize=True)\n",
      "    3.0    0.4\n",
      "    1.0    0.2\n",
      "    2.0    0.2\n",
      "    4.0    0.2\n",
      "    dtype: float64\n",
      "    \n",
      "    **bins**\n",
      "    \n",
      "    Bins can be useful for going from a continuous variable to a\n",
      "    categorical variable; instead of counting unique\n",
      "    apparitions of values, divide the index in the specified\n",
      "    number of half-open bins.\n",
      "    \n",
      "    >>> s.value_counts(bins=3)\n",
      "    (0.996, 2.0]    2\n",
      "    (2.0, 3.0]      2\n",
      "    (3.0, 4.0]      1\n",
      "    dtype: int64\n",
      "    \n",
      "    **dropna**\n",
      "    \n",
      "    With `dropna` set to `False` we can also see NaN index values.\n",
      "    \n",
      "    >>> s.value_counts(dropna=False)\n",
      "    3.0    2\n",
      "    1.0    1\n",
      "    2.0    1\n",
      "    4.0    1\n",
      "    NaN    1\n",
      "    dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "help(pd.Series.value_counts)    # pd의 객체의 메서드(함수) 사용법 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['calibration', 'cluster', 'covariance', 'cross_decomposition', 'datasets', 'decomposition', 'dummy', 'ensemble', 'exceptions', 'experimental', 'externals', 'feature_extraction', 'feature_selection', 'gaussian_process', 'inspection', 'isotonic', 'kernel_approximation', 'kernel_ridge', 'linear_model', 'manifold', 'metrics', 'mixture', 'model_selection', 'multiclass', 'multioutput', 'naive_bayes', 'neighbors', 'neural_network', 'pipeline', 'preprocessing', 'random_projection', 'semi_supervised', 'svm', 'tree', 'discriminant_analysis', 'impute', 'compose', 'clone', 'get_config', 'set_config', 'config_context', 'show_versions']\n",
      "Help on package sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn\n",
      "\n",
      "DESCRIPTION\n",
      "    Machine learning module for Python\n",
      "    ==================================\n",
      "    \n",
      "    sklearn is a Python module integrating classical machine\n",
      "    learning algorithms in the tightly-knit world of scientific Python\n",
      "    packages (numpy, scipy, matplotlib).\n",
      "    \n",
      "    It aims to provide simple and efficient solutions to learning problems\n",
      "    that are accessible to everybody and reusable in various contexts:\n",
      "    machine-learning as a versatile tool for science and engineering.\n",
      "    \n",
      "    See http://scikit-learn.org for complete documentation.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    __check_build (package)\n",
      "    _build_utils (package)\n",
      "    _config\n",
      "    _distributor_init\n",
      "    _isotonic\n",
      "    _loss (package)\n",
      "    _min_dependencies\n",
      "    base\n",
      "    calibration\n",
      "    cluster (package)\n",
      "    compose (package)\n",
      "    conftest\n",
      "    covariance (package)\n",
      "    cross_decomposition (package)\n",
      "    datasets (package)\n",
      "    decomposition (package)\n",
      "    discriminant_analysis\n",
      "    dummy\n",
      "    ensemble (package)\n",
      "    exceptions\n",
      "    experimental (package)\n",
      "    externals (package)\n",
      "    feature_extraction (package)\n",
      "    feature_selection (package)\n",
      "    gaussian_process (package)\n",
      "    impute (package)\n",
      "    inspection (package)\n",
      "    isotonic\n",
      "    kernel_approximation\n",
      "    kernel_ridge\n",
      "    linear_model (package)\n",
      "    manifold (package)\n",
      "    metrics (package)\n",
      "    mixture (package)\n",
      "    model_selection (package)\n",
      "    multiclass\n",
      "    multioutput\n",
      "    naive_bayes\n",
      "    neighbors (package)\n",
      "    neural_network (package)\n",
      "    pipeline\n",
      "    preprocessing (package)\n",
      "    random_projection\n",
      "    semi_supervised (package)\n",
      "    svm (package)\n",
      "    tests (package)\n",
      "    tree (package)\n",
      "    utils (package)\n",
      "\n",
      "FUNCTIONS\n",
      "    clone(estimator, *, safe=True)\n",
      "        Construct a new unfitted estimator with the same parameters.\n",
      "        \n",
      "        Clone does a deep copy of the model in an estimator\n",
      "        without actually copying attached data. It returns a new estimator\n",
      "        with the same parameters that has not been fitted on any data.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : {list, tuple, set} of estimator instance or a single             estimator instance\n",
      "            The estimator or group of estimators to be cloned.\n",
      "        safe : bool, default=True\n",
      "            If safe is False, clone will fall back to a deep copy on objects\n",
      "            that are not estimators.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        estimator : object\n",
      "            The deep copy of the input, an estimator if input is an estimator.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        If the estimator's `random_state` parameter is an integer (or if the\n",
      "        estimator doesn't have a `random_state` parameter), an *exact clone* is\n",
      "        returned: the clone and the original estimator will give the exact same\n",
      "        results. Otherwise, *statistical clone* is returned: the clone might\n",
      "        return different results from the original estimator. More details can be\n",
      "        found in :ref:`randomness`.\n",
      "    \n",
      "    config_context(*, assume_finite=None, working_memory=None, print_changed_only=None, display=None, pairwise_dist_chunk_size=None, enable_cython_pairwise_dist=None, array_api_dispatch=None, transform_output=None)\n",
      "        Context manager for global scikit-learn configuration.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        assume_finite : bool, default=None\n",
      "            If True, validation for finiteness will be skipped,\n",
      "            saving time, but leading to potential crashes. If\n",
      "            False, validation for finiteness will be performed,\n",
      "            avoiding error. If None, the existing value won't change.\n",
      "            The default value is False.\n",
      "        \n",
      "        working_memory : int, default=None\n",
      "            If set, scikit-learn will attempt to limit the size of temporary arrays\n",
      "            to this number of MiB (per job when parallelised), often saving both\n",
      "            computation time and memory on expensive operations that can be\n",
      "            performed in chunks. If None, the existing value won't change.\n",
      "            The default value is 1024.\n",
      "        \n",
      "        print_changed_only : bool, default=None\n",
      "            If True, only the parameters that were set to non-default\n",
      "            values will be printed when printing an estimator. For example,\n",
      "            ``print(SVC())`` while True will only print 'SVC()', but would print\n",
      "            'SVC(C=1.0, cache_size=200, ...)' with all the non-changed parameters\n",
      "            when False. If None, the existing value won't change.\n",
      "            The default value is True.\n",
      "        \n",
      "            .. versionchanged:: 0.23\n",
      "               Default changed from False to True.\n",
      "        \n",
      "        display : {'text', 'diagram'}, default=None\n",
      "            If 'diagram', estimators will be displayed as a diagram in a Jupyter\n",
      "            lab or notebook context. If 'text', estimators will be displayed as\n",
      "            text. If None, the existing value won't change.\n",
      "            The default value is 'diagram'.\n",
      "        \n",
      "            .. versionadded:: 0.23\n",
      "        \n",
      "        pairwise_dist_chunk_size : int, default=None\n",
      "            The number of row vectors per chunk for the accelerated pairwise-\n",
      "            distances reduction backend. Default is 256 (suitable for most of\n",
      "            modern laptops' caches and architectures).\n",
      "        \n",
      "            Intended for easier benchmarking and testing of scikit-learn internals.\n",
      "            End users are not expected to benefit from customizing this configuration\n",
      "            setting.\n",
      "        \n",
      "            .. versionadded:: 1.1\n",
      "        \n",
      "        enable_cython_pairwise_dist : bool, default=None\n",
      "            Use the accelerated pairwise-distances reduction backend when\n",
      "            possible. Global default: True.\n",
      "        \n",
      "            Intended for easier benchmarking and testing of scikit-learn internals.\n",
      "            End users are not expected to benefit from customizing this configuration\n",
      "            setting.\n",
      "        \n",
      "            .. versionadded:: 1.1\n",
      "        \n",
      "        array_api_dispatch : bool, default=None\n",
      "            Use Array API dispatching when inputs follow the Array API standard.\n",
      "            Default is False.\n",
      "        \n",
      "            See the :ref:`User Guide <array_api>` for more details.\n",
      "        \n",
      "            .. versionadded:: 1.2\n",
      "        \n",
      "        transform_output : str, default=None\n",
      "            Configure output of `transform` and `fit_transform`.\n",
      "        \n",
      "            See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n",
      "            for an example on how to use the API.\n",
      "        \n",
      "            - `\"default\"`: Default output format of a transformer\n",
      "            - `\"pandas\"`: DataFrame output\n",
      "            - `None`: Transform configuration is unchanged\n",
      "        \n",
      "            .. versionadded:: 1.2\n",
      "        \n",
      "        Yields\n",
      "        ------\n",
      "        None.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        set_config : Set global scikit-learn configuration.\n",
      "        get_config : Retrieve current values of the global configuration.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        All settings, not just those presently modified, will be returned to\n",
      "        their previous values when the context manager is exited.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import sklearn\n",
      "        >>> from sklearn.utils.validation import assert_all_finite\n",
      "        >>> with sklearn.config_context(assume_finite=True):\n",
      "        ...     assert_all_finite([float('nan')])\n",
      "        >>> with sklearn.config_context(assume_finite=True):\n",
      "        ...     with sklearn.config_context(assume_finite=False):\n",
      "        ...         assert_all_finite([float('nan')])\n",
      "        Traceback (most recent call last):\n",
      "        ...\n",
      "        ValueError: Input contains NaN...\n",
      "    \n",
      "    get_config()\n",
      "        Retrieve current values for configuration set by :func:`set_config`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        config : dict\n",
      "            Keys are parameter names that can be passed to :func:`set_config`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        config_context : Context manager for global scikit-learn configuration.\n",
      "        set_config : Set global scikit-learn configuration.\n",
      "    \n",
      "    set_config(assume_finite=None, working_memory=None, print_changed_only=None, display=None, pairwise_dist_chunk_size=None, enable_cython_pairwise_dist=None, array_api_dispatch=None, transform_output=None)\n",
      "        Set global scikit-learn configuration\n",
      "        \n",
      "        .. versionadded:: 0.19\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        assume_finite : bool, default=None\n",
      "            If True, validation for finiteness will be skipped,\n",
      "            saving time, but leading to potential crashes. If\n",
      "            False, validation for finiteness will be performed,\n",
      "            avoiding error.  Global default: False.\n",
      "        \n",
      "            .. versionadded:: 0.19\n",
      "        \n",
      "        working_memory : int, default=None\n",
      "            If set, scikit-learn will attempt to limit the size of temporary arrays\n",
      "            to this number of MiB (per job when parallelised), often saving both\n",
      "            computation time and memory on expensive operations that can be\n",
      "            performed in chunks. Global default: 1024.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        print_changed_only : bool, default=None\n",
      "            If True, only the parameters that were set to non-default\n",
      "            values will be printed when printing an estimator. For example,\n",
      "            ``print(SVC())`` while True will only print 'SVC()' while the default\n",
      "            behaviour would be to print 'SVC(C=1.0, cache_size=200, ...)' with\n",
      "            all the non-changed parameters.\n",
      "        \n",
      "            .. versionadded:: 0.21\n",
      "        \n",
      "        display : {'text', 'diagram'}, default=None\n",
      "            If 'diagram', estimators will be displayed as a diagram in a Jupyter\n",
      "            lab or notebook context. If 'text', estimators will be displayed as\n",
      "            text. Default is 'diagram'.\n",
      "        \n",
      "            .. versionadded:: 0.23\n",
      "        \n",
      "        pairwise_dist_chunk_size : int, default=None\n",
      "            The number of row vectors per chunk for the accelerated pairwise-\n",
      "            distances reduction backend. Default is 256 (suitable for most of\n",
      "            modern laptops' caches and architectures).\n",
      "        \n",
      "            Intended for easier benchmarking and testing of scikit-learn internals.\n",
      "            End users are not expected to benefit from customizing this configuration\n",
      "            setting.\n",
      "        \n",
      "            .. versionadded:: 1.1\n",
      "        \n",
      "        enable_cython_pairwise_dist : bool, default=None\n",
      "            Use the accelerated pairwise-distances reduction backend when\n",
      "            possible. Global default: True.\n",
      "        \n",
      "            Intended for easier benchmarking and testing of scikit-learn internals.\n",
      "            End users are not expected to benefit from customizing this configuration\n",
      "            setting.\n",
      "        \n",
      "            .. versionadded:: 1.1\n",
      "        \n",
      "        array_api_dispatch : bool, default=None\n",
      "            Use Array API dispatching when inputs follow the Array API standard.\n",
      "            Default is False.\n",
      "        \n",
      "            See the :ref:`User Guide <array_api>` for more details.\n",
      "        \n",
      "            .. versionadded:: 1.2\n",
      "        \n",
      "        transform_output : str, default=None\n",
      "            Configure output of `transform` and `fit_transform`.\n",
      "        \n",
      "            See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n",
      "            for an example on how to use the API.\n",
      "        \n",
      "            - `\"default\"`: Default output format of a transformer\n",
      "            - `\"pandas\"`: DataFrame output\n",
      "            - `None`: Transform configuration is unchanged\n",
      "        \n",
      "            .. versionadded:: 1.2\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        config_context : Context manager for global scikit-learn configuration.\n",
      "        get_config : Retrieve current values of the global configuration.\n",
      "    \n",
      "    show_versions()\n",
      "        Print useful debugging information\"\n",
      "        \n",
      "        .. versionadded:: 0.20\n",
      "\n",
      "DATA\n",
      "    __SKLEARN_SETUP__ = False\n",
      "    __all__ = ['calibration', 'cluster', 'covariance', 'cross_decompositio...\n",
      "\n",
      "VERSION\n",
      "    1.2.2\n",
      "\n",
      "FILE\n",
      "    c:\\users\\lee\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\__init__.py\n",
      "\n",
      "\n",
      "None\n",
      "['ARDRegression', 'BayesianRidge', 'ElasticNet', 'ElasticNetCV', 'GammaRegressor', 'Hinge', 'Huber', 'HuberRegressor', 'Lars', 'LarsCV', 'Lasso', 'LassoCV', 'LassoLars', 'LassoLarsCV', 'LassoLarsIC', 'LinearRegression', 'Log', 'LogisticRegression', 'LogisticRegressionCV', 'ModifiedHuber', 'MultiTaskElasticNet', 'MultiTaskElasticNetCV', 'MultiTaskLasso', 'MultiTaskLassoCV', 'OrthogonalMatchingPursuit', 'OrthogonalMatchingPursuitCV', 'PassiveAggressiveClassifier', 'PassiveAggressiveRegressor', 'Perceptron', 'PoissonRegressor', 'QuantileRegressor', 'RANSACRegressor', 'Ridge', 'RidgeCV', 'RidgeClassifier', 'RidgeClassifierCV', 'SGDClassifier', 'SGDOneClassSVM', 'SGDRegressor', 'SquaredLoss', 'TheilSenRegressor', 'TweedieRegressor', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_base', '_bayes', '_cd_fast', '_coordinate_descent', '_glm', '_huber', '_least_angle', '_linear_loss', '_logistic', '_omp', '_passive_aggressive', '_perceptron', '_quantile', '_ransac', '_ridge', '_sag', '_sag_fast', '_sgd_fast', '_stochastic_gradient', '_theil_sen', 'enet_path', 'lars_path', 'lars_path_gram', 'lasso_path', 'orthogonal_mp', 'orthogonal_mp_gram', 'ridge_regression']\n",
      "Help on class LogisticRegression in module sklearn.linear_model._logistic:\n",
      "\n",
      "class LogisticRegression(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n",
      " |  LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
      " |  \n",
      " |  Logistic Regression (aka logit, MaxEnt) classifier.\n",
      " |  \n",
      " |  In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
      " |  scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
      " |  cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
      " |  (Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
      " |  'sag', 'saga' and 'newton-cg' solvers.)\n",
      " |  \n",
      " |  This class implements regularized logistic regression using the\n",
      " |  'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
      " |  that regularization is applied by default**. It can handle both dense\n",
      " |  and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
      " |  floats for optimal performance; any other input format will be converted\n",
      " |  (and copied).\n",
      " |  \n",
      " |  The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
      " |  with primal formulation, or no regularization. The 'liblinear' solver\n",
      " |  supports both L1 and L2 regularization, with a dual formulation only for\n",
      " |  the L2 penalty. The Elastic-Net regularization is only supported by the\n",
      " |  'saga' solver.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <logistic_regression>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  penalty : {'l1', 'l2', 'elasticnet', None}, default='l2'\n",
      " |      Specify the norm of the penalty:\n",
      " |  \n",
      " |      - `None`: no penalty is added;\n",
      " |      - `'l2'`: add a L2 penalty term and it is the default choice;\n",
      " |      - `'l1'`: add a L1 penalty term;\n",
      " |      - `'elasticnet'`: both L1 and L2 penalty terms are added.\n",
      " |  \n",
      " |      .. warning::\n",
      " |         Some penalties may not work with some solvers. See the parameter\n",
      " |         `solver` below, to know the compatibility between the penalty and\n",
      " |         solver.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |         l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
      " |  \n",
      " |      .. deprecated:: 1.2\n",
      " |         The 'none' option was deprecated in version 1.2, and will be removed\n",
      " |         in 1.4. Use `None` instead.\n",
      " |  \n",
      " |  dual : bool, default=False\n",
      " |      Dual or primal formulation. Dual formulation is only implemented for\n",
      " |      l2 penalty with liblinear solver. Prefer dual=False when\n",
      " |      n_samples > n_features.\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      Tolerance for stopping criteria.\n",
      " |  \n",
      " |  C : float, default=1.0\n",
      " |      Inverse of regularization strength; must be a positive float.\n",
      " |      Like in support vector machines, smaller values specify stronger\n",
      " |      regularization.\n",
      " |  \n",
      " |  fit_intercept : bool, default=True\n",
      " |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      " |      added to the decision function.\n",
      " |  \n",
      " |  intercept_scaling : float, default=1\n",
      " |      Useful only when the solver 'liblinear' is used\n",
      " |      and self.fit_intercept is set to True. In this case, x becomes\n",
      " |      [x, self.intercept_scaling],\n",
      " |      i.e. a \"synthetic\" feature with constant value equal to\n",
      " |      intercept_scaling is appended to the instance vector.\n",
      " |      The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
      " |  \n",
      " |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      " |      as all other features.\n",
      " |      To lessen the effect of regularization on synthetic feature weight\n",
      " |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      " |  \n",
      " |  class_weight : dict or 'balanced', default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one.\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *class_weight='balanced'*\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
      " |      data. See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'\n",
      " |  \n",
      " |      Algorithm to use in the optimization problem. Default is 'lbfgs'.\n",
      " |      To choose a solver, you might want to consider the following aspects:\n",
      " |  \n",
      " |          - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n",
      " |            and 'saga' are faster for large ones;\n",
      " |          - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n",
      " |            'lbfgs' handle multinomial loss;\n",
      " |          - 'liblinear' is limited to one-versus-rest schemes.\n",
      " |          - 'newton-cholesky' is a good choice for `n_samples` >> `n_features`,\n",
      " |            especially with one-hot encoded categorical features with rare\n",
      " |            categories. Note that it is limited to binary classification and the\n",
      " |            one-versus-rest reduction for multiclass classification. Be aware that\n",
      " |            the memory usage of this solver has a quadratic dependency on\n",
      " |            `n_features` because it explicitly computes the Hessian matrix.\n",
      " |  \n",
      " |      .. warning::\n",
      " |         The choice of the algorithm depends on the penalty chosen.\n",
      " |         Supported penalties by solver:\n",
      " |  \n",
      " |         - 'lbfgs'           -   ['l2', None]\n",
      " |         - 'liblinear'       -   ['l1', 'l2']\n",
      " |         - 'newton-cg'       -   ['l2', None]\n",
      " |         - 'newton-cholesky' -   ['l2', None]\n",
      " |         - 'sag'             -   ['l2', None]\n",
      " |         - 'saga'            -   ['elasticnet', 'l1', 'l2', None]\n",
      " |  \n",
      " |      .. note::\n",
      " |         'sag' and 'saga' fast convergence is only guaranteed on features\n",
      " |         with approximately the same scale. You can preprocess the data with\n",
      " |         a scaler from :mod:`sklearn.preprocessing`.\n",
      " |  \n",
      " |      .. seealso::\n",
      " |         Refer to the User Guide for more information regarding\n",
      " |         :class:`LogisticRegression` and more specifically the\n",
      " |         :ref:`Table <Logistic_regression>`\n",
      " |         summarizing solver/penalty supports.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         Stochastic Average Gradient descent solver.\n",
      " |      .. versionadded:: 0.19\n",
      " |         SAGA solver.\n",
      " |      .. versionchanged:: 0.22\n",
      " |          The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
      " |      .. versionadded:: 1.2\n",
      " |         newton-cholesky solver.\n",
      " |  \n",
      " |  max_iter : int, default=100\n",
      " |      Maximum number of iterations taken for the solvers to converge.\n",
      " |  \n",
      " |  multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
      " |      If the option chosen is 'ovr', then a binary problem is fit for each\n",
      " |      label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
      " |      across the entire probability distribution, *even when the data is\n",
      " |      binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
      " |      'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
      " |      and otherwise selects 'multinomial'.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |         Stochastic Average Gradient descent solver for 'multinomial' case.\n",
      " |      .. versionchanged:: 0.22\n",
      " |          Default changed from 'ovr' to 'auto' in 0.22.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      For the liblinear and lbfgs solvers set verbose to any positive\n",
      " |      number for verbosity.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to True, reuse the solution of the previous call to fit as\n",
      " |      initialization, otherwise, just erase the previous solution.\n",
      " |      Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      Number of CPU cores used when parallelizing over classes if\n",
      " |      multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
      " |      set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
      " |      not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      " |      context. ``-1`` means using all processors.\n",
      " |      See :term:`Glossary <n_jobs>` for more details.\n",
      " |  \n",
      " |  l1_ratio : float, default=None\n",
      " |      The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
      " |      used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
      " |      to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
      " |      to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
      " |      combination of L1 and L2.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes, )\n",
      " |      A list of class labels known to the classifier.\n",
      " |  \n",
      " |  coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
      " |      Coefficient of the features in the decision function.\n",
      " |  \n",
      " |      `coef_` is of shape (1, n_features) when the given problem is binary.\n",
      " |      In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
      " |      to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
      " |  \n",
      " |  intercept_ : ndarray of shape (1,) or (n_classes,)\n",
      " |      Intercept (a.k.a. bias) added to the decision function.\n",
      " |  \n",
      " |      If `fit_intercept` is set to False, the intercept is set to zero.\n",
      " |      `intercept_` is of shape (1,) when the given problem is binary.\n",
      " |      In particular, when `multi_class='multinomial'`, `intercept_`\n",
      " |      corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
      " |      outcome 0 (False).\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
      " |      Actual number of iterations for all classes. If binary or multinomial,\n",
      " |      it returns only 1 element. For liblinear solver, only the maximum\n",
      " |      number of iteration across all classes is given.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |  \n",
      " |          In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
      " |          ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  SGDClassifier : Incrementally trained logistic regression (when given\n",
      " |      the parameter ``loss=\"log\"``).\n",
      " |  LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The underlying C implementation uses a random number generator to\n",
      " |  select features when fitting the model. It is thus not uncommon,\n",
      " |  to have slightly different results for the same input data. If\n",
      " |  that happens, try with a smaller tol parameter.\n",
      " |  \n",
      " |  Predict output may not match that of standalone liblinear in certain\n",
      " |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      " |  in the narrative documentation.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
      " |      Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
      " |      http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
      " |  \n",
      " |  LIBLINEAR -- A Library for Large Linear Classification\n",
      " |      https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
      " |  \n",
      " |  SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
      " |      Minimizing Finite Sums with the Stochastic Average Gradient\n",
      " |      https://hal.inria.fr/hal-00860051/document\n",
      " |  \n",
      " |  SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
      " |          :arxiv:`\"SAGA: A Fast Incremental Gradient Method With Support\n",
      " |          for Non-Strongly Convex Composite Objectives\" <1407.0202>`\n",
      " |  \n",
      " |  Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
      " |      methods for logistic regression and maximum entropy models.\n",
      " |      Machine Learning 85(1-2):41-75.\n",
      " |      https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_iris\n",
      " |  >>> from sklearn.linear_model import LogisticRegression\n",
      " |  >>> X, y = load_iris(return_X_y=True)\n",
      " |  >>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
      " |  >>> clf.predict(X[:2, :])\n",
      " |  array([0, 0])\n",
      " |  >>> clf.predict_proba(X[:2, :])\n",
      " |  array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
      " |         [9.7...e-01, 2.8...e-02, ...e-08]])\n",
      " |  >>> clf.score(X, y)\n",
      " |  0.97...\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LogisticRegression\n",
      " |      sklearn.linear_model._base.LinearClassifierMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.linear_model._base.SparseCoefMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the model according to the given training data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vector, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target vector relative to X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,) default=None\n",
      " |          Array of weights that are assigned to individual samples.\n",
      " |          If not provided, then each sample is given unit weight.\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |             *sample_weight* support to LogisticRegression.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The SAGA solver supports both float64 and float32 bit arrays.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict logarithm of probability estimates.\n",
      " |      \n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Vector to be scored, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the log-probability of the sample for each class in the\n",
      " |          model, where classes are ordered as they are in ``self.classes_``.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Probability estimates.\n",
      " |      \n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |      \n",
      " |      For a multi_class problem, if multi_class is set to be \"multinomial\"\n",
      " |      the softmax function is used to find the predicted probability of\n",
      " |      each class.\n",
      " |      Else use a one-vs-rest approach, i.e calculate the probability\n",
      " |      of each class assuming it to be positive using the logistic function.\n",
      " |      and normalize these values across all the classes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Vector to be scored, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the probability of the sample for each class in the model,\n",
      " |          where classes are ordered as they are in ``self.classes_``.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Predict confidence scores for samples.\n",
      " |      \n",
      " |      The confidence score for a sample is proportional to the signed\n",
      " |      distance of that sample to the hyperplane.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The data matrix for which we want to get the confidence scores.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      " |          Confidence scores per `(n_samples, n_classes)` combination. In the\n",
      " |          binary case, confidence score for `self.classes_[1]` where >0 means\n",
      " |          this class would be predicted.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class labels for samples in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The data matrix for which we want to get the predictions.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_pred : ndarray of shape (n_samples,)\n",
      " |          Vector containing the class labels for each sample.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      " |  \n",
      " |  densify(self)\n",
      " |      Convert coefficient matrix to dense array format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      " |      default format of ``coef_`` and is required for fitting, so calling\n",
      " |      this method is only required on models that have previously been\n",
      " |      sparsified; otherwise, it is a no-op.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  sparsify(self)\n",
      " |      Convert coefficient matrix to sparse format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      " |      L1-regularized models can be much more memory- and storage-efficient\n",
      " |      than the usual numpy.ndarray representation.\n",
      " |      \n",
      " |      The ``intercept_`` member is not converted.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      " |      this may actually *increase* memory usage, so use this method with\n",
      " |      care. A rule of thumb is that the number of zero elements, which can\n",
      " |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      " |      to provide significant benefits.\n",
      " |      \n",
      " |      After calling this method, further fitting with the partial_fit\n",
      " |      method (if any) will not work until you call densify.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n",
      "None\n",
      "Help on function replace in module pandas.core.series:\n",
      "\n",
      "replace(self, to_replace=None, value=<no_default>, *, inplace: 'bool' = False, limit: 'int | None' = None, regex: 'bool' = False, method: \"Literal[('pad', 'ffill', 'bfill')] | lib.NoDefault\" = <no_default>) -> 'Series | None'\n",
      "    Replace values given in `to_replace` with `value`.\n",
      "    \n",
      "    Values of the Series are replaced with other values dynamically.\n",
      "    \n",
      "    This differs from updating with ``.loc`` or ``.iloc``, which require\n",
      "    you to specify a location to update with some value.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    to_replace : str, regex, list, dict, Series, int, float, or None\n",
      "        How to find the values that will be replaced.\n",
      "    \n",
      "        * numeric, str or regex:\n",
      "    \n",
      "            - numeric: numeric values equal to `to_replace` will be\n",
      "              replaced with `value`\n",
      "            - str: string exactly matching `to_replace` will be replaced\n",
      "              with `value`\n",
      "            - regex: regexs matching `to_replace` will be replaced with\n",
      "              `value`\n",
      "    \n",
      "        * list of str, regex, or numeric:\n",
      "    \n",
      "            - First, if `to_replace` and `value` are both lists, they\n",
      "              **must** be the same length.\n",
      "            - Second, if ``regex=True`` then all of the strings in **both**\n",
      "              lists will be interpreted as regexs otherwise they will match\n",
      "              directly. This doesn't matter much for `value` since there\n",
      "              are only a few possible substitution regexes you can use.\n",
      "            - str, regex and numeric rules apply as above.\n",
      "    \n",
      "        * dict:\n",
      "    \n",
      "            - Dicts can be used to specify different replacement values\n",
      "              for different existing values. For example,\n",
      "              ``{'a': 'b', 'y': 'z'}`` replaces the value 'a' with 'b' and\n",
      "              'y' with 'z'. To use a dict in this way, the optional `value`\n",
      "              parameter should not be given.\n",
      "            - For a DataFrame a dict can specify that different values\n",
      "              should be replaced in different columns. For example,\n",
      "              ``{'a': 1, 'b': 'z'}`` looks for the value 1 in column 'a'\n",
      "              and the value 'z' in column 'b' and replaces these values\n",
      "              with whatever is specified in `value`. The `value` parameter\n",
      "              should not be ``None`` in this case. You can treat this as a\n",
      "              special case of passing two lists except that you are\n",
      "              specifying the column to search in.\n",
      "            - For a DataFrame nested dictionaries, e.g.,\n",
      "              ``{'a': {'b': np.nan}}``, are read as follows: look in column\n",
      "              'a' for the value 'b' and replace it with NaN. The optional `value`\n",
      "              parameter should not be specified to use a nested dict in this\n",
      "              way. You can nest regular expressions as well. Note that\n",
      "              column names (the top-level dictionary keys in a nested\n",
      "              dictionary) **cannot** be regular expressions.\n",
      "    \n",
      "        * None:\n",
      "    \n",
      "            - This means that the `regex` argument must be a string,\n",
      "              compiled regular expression, or list, dict, ndarray or\n",
      "              Series of such elements. If `value` is also ``None`` then\n",
      "              this **must** be a nested dictionary or Series.\n",
      "    \n",
      "        See the examples section for examples of each of these.\n",
      "    value : scalar, dict, list, str, regex, default None\n",
      "        Value to replace any values matching `to_replace` with.\n",
      "        For a DataFrame a dict of values can be used to specify which\n",
      "        value to use for each column (columns not in the dict will not be\n",
      "        filled). Regular expressions, strings and lists or dicts of such\n",
      "        objects are also allowed.\n",
      "    inplace : bool, default False\n",
      "        If True, performs operation inplace and returns None.\n",
      "    limit : int, default None\n",
      "        Maximum size gap to forward or backward fill.\n",
      "    regex : bool or same types as `to_replace`, default False\n",
      "        Whether to interpret `to_replace` and/or `value` as regular\n",
      "        expressions. If this is ``True`` then `to_replace` *must* be a\n",
      "        string. Alternatively, this could be a regular expression or a\n",
      "        list, dict, or array of regular expressions in which case\n",
      "        `to_replace` must be ``None``.\n",
      "    method : {'pad', 'ffill', 'bfill'}\n",
      "        The method to use when for replacement, when `to_replace` is a\n",
      "        scalar, list or tuple and `value` is ``None``.\n",
      "    \n",
      "        .. versionchanged:: 0.23.0\n",
      "            Added to DataFrame.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    Series\n",
      "        Object after replacement.\n",
      "    \n",
      "    Raises\n",
      "    ------\n",
      "    AssertionError\n",
      "        * If `regex` is not a ``bool`` and `to_replace` is not\n",
      "          ``None``.\n",
      "    \n",
      "    TypeError\n",
      "        * If `to_replace` is not a scalar, array-like, ``dict``, or ``None``\n",
      "        * If `to_replace` is a ``dict`` and `value` is not a ``list``,\n",
      "          ``dict``, ``ndarray``, or ``Series``\n",
      "        * If `to_replace` is ``None`` and `regex` is not compilable\n",
      "          into a regular expression or is a list, dict, ndarray, or\n",
      "          Series.\n",
      "        * When replacing multiple ``bool`` or ``datetime64`` objects and\n",
      "          the arguments to `to_replace` does not match the type of the\n",
      "          value being replaced\n",
      "    \n",
      "    ValueError\n",
      "        * If a ``list`` or an ``ndarray`` is passed to `to_replace` and\n",
      "          `value` but they are not the same length.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    Series.fillna : Fill NA values.\n",
      "    Series.where : Replace values based on boolean condition.\n",
      "    Series.str.replace : Simple string replacement.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    * Regex substitution is performed under the hood with ``re.sub``. The\n",
      "      rules for substitution for ``re.sub`` are the same.\n",
      "    * Regular expressions will only substitute on strings, meaning you\n",
      "      cannot provide, for example, a regular expression matching floating\n",
      "      point numbers and expect the columns in your frame that have a\n",
      "      numeric dtype to be matched. However, if those floating point\n",
      "      numbers *are* strings, then you can do this.\n",
      "    * This method has *a lot* of options. You are encouraged to experiment\n",
      "      and play with this method to gain intuition about how it works.\n",
      "    * When dict is used as the `to_replace` value, it is like\n",
      "      key(s) in the dict are the to_replace part and\n",
      "      value(s) in the dict are the value parameter.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    \n",
      "    **Scalar `to_replace` and `value`**\n",
      "    \n",
      "    >>> s = pd.Series([1, 2, 3, 4, 5])\n",
      "    >>> s.replace(1, 5)\n",
      "    0    5\n",
      "    1    2\n",
      "    2    3\n",
      "    3    4\n",
      "    4    5\n",
      "    dtype: int64\n",
      "    \n",
      "    >>> df = pd.DataFrame({'A': [0, 1, 2, 3, 4],\n",
      "    ...                    'B': [5, 6, 7, 8, 9],\n",
      "    ...                    'C': ['a', 'b', 'c', 'd', 'e']})\n",
      "    >>> df.replace(0, 5)\n",
      "        A  B  C\n",
      "    0  5  5  a\n",
      "    1  1  6  b\n",
      "    2  2  7  c\n",
      "    3  3  8  d\n",
      "    4  4  9  e\n",
      "    \n",
      "    **List-like `to_replace`**\n",
      "    \n",
      "    >>> df.replace([0, 1, 2, 3], 4)\n",
      "        A  B  C\n",
      "    0  4  5  a\n",
      "    1  4  6  b\n",
      "    2  4  7  c\n",
      "    3  4  8  d\n",
      "    4  4  9  e\n",
      "    \n",
      "    >>> df.replace([0, 1, 2, 3], [4, 3, 2, 1])\n",
      "        A  B  C\n",
      "    0  4  5  a\n",
      "    1  3  6  b\n",
      "    2  2  7  c\n",
      "    3  1  8  d\n",
      "    4  4  9  e\n",
      "    \n",
      "    >>> s.replace([1, 2], method='bfill')\n",
      "    0    3\n",
      "    1    3\n",
      "    2    3\n",
      "    3    4\n",
      "    4    5\n",
      "    dtype: int64\n",
      "    \n",
      "    **dict-like `to_replace`**\n",
      "    \n",
      "    >>> df.replace({0: 10, 1: 100})\n",
      "            A  B  C\n",
      "    0   10  5  a\n",
      "    1  100  6  b\n",
      "    2    2  7  c\n",
      "    3    3  8  d\n",
      "    4    4  9  e\n",
      "    \n",
      "    >>> df.replace({'A': 0, 'B': 5}, 100)\n",
      "            A    B  C\n",
      "    0  100  100  a\n",
      "    1    1    6  b\n",
      "    2    2    7  c\n",
      "    3    3    8  d\n",
      "    4    4    9  e\n",
      "    \n",
      "    >>> df.replace({'A': {0: 100, 4: 400}})\n",
      "            A  B  C\n",
      "    0  100  5  a\n",
      "    1    1  6  b\n",
      "    2    2  7  c\n",
      "    3    3  8  d\n",
      "    4  400  9  e\n",
      "    \n",
      "    **Regular expression `to_replace`**\n",
      "    \n",
      "    >>> df = pd.DataFrame({'A': ['bat', 'foo', 'bait'],\n",
      "    ...                    'B': ['abc', 'bar', 'xyz']})\n",
      "    >>> df.replace(to_replace=r'^ba.$', value='new', regex=True)\n",
      "            A    B\n",
      "    0   new  abc\n",
      "    1   foo  new\n",
      "    2  bait  xyz\n",
      "    \n",
      "    >>> df.replace({'A': r'^ba.$'}, {'A': 'new'}, regex=True)\n",
      "            A    B\n",
      "    0   new  abc\n",
      "    1   foo  bar\n",
      "    2  bait  xyz\n",
      "    \n",
      "    >>> df.replace(regex=r'^ba.$', value='new')\n",
      "            A    B\n",
      "    0   new  abc\n",
      "    1   foo  new\n",
      "    2  bait  xyz\n",
      "    \n",
      "    >>> df.replace(regex={r'^ba.$': 'new', 'foo': 'xyz'})\n",
      "            A    B\n",
      "    0   new  abc\n",
      "    1   xyz  new\n",
      "    2  bait  xyz\n",
      "    \n",
      "    >>> df.replace(regex=[r'^ba.$', 'foo'], value='new')\n",
      "            A    B\n",
      "    0   new  abc\n",
      "    1   new  new\n",
      "    2  bait  xyz\n",
      "    \n",
      "    Compare the behavior of ``s.replace({'a': None})`` and\n",
      "    ``s.replace('a', None)`` to understand the peculiarities\n",
      "    of the `to_replace` parameter:\n",
      "    \n",
      "    >>> s = pd.Series([10, 'a', 'a', 'b', 'a'])\n",
      "    \n",
      "    When one uses a dict as the `to_replace` value, it is like the\n",
      "    value(s) in the dict are equal to the `value` parameter.\n",
      "    ``s.replace({'a': None})`` is equivalent to\n",
      "    ``s.replace(to_replace={'a': None}, value=None, method=None)``:\n",
      "    \n",
      "    >>> s.replace({'a': None})\n",
      "    0      10\n",
      "    1    None\n",
      "    2    None\n",
      "    3       b\n",
      "    4    None\n",
      "    dtype: object\n",
      "    \n",
      "    When ``value`` is not explicitly passed and `to_replace` is a scalar, list\n",
      "    or tuple, `replace` uses the method parameter (default 'pad') to do the\n",
      "    replacement. So this is why the 'a' values are being replaced by 10\n",
      "    in rows 1 and 2 and 'b' in row 4 in this case.\n",
      "    \n",
      "    >>> s.replace('a')\n",
      "    0    10\n",
      "    1    10\n",
      "    2    10\n",
      "    3     b\n",
      "    4     b\n",
      "    dtype: object\n",
      "    \n",
      "    On the other hand, if ``None`` is explicitly passed for ``value``, it will\n",
      "    be respected:\n",
      "    \n",
      "    >>> s.replace('a', None)\n",
      "    0      10\n",
      "    1    None\n",
      "    2    None\n",
      "    3       b\n",
      "    4    None\n",
      "    dtype: object\n",
      "    \n",
      "        .. versionchanged:: 1.4.0\n",
      "            Previously the explicit ``None`` was silently ignored.\n",
      "\n",
      "None\n",
      "<class 'property'>\n",
      "<class 'function'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import sklearn \n",
    "import sklearn.linear_model \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "print(sklearn.__all__)\n",
    "\n",
    "print(help(sklearn))    # sklearn의 패키지 철자가 생각나지 않을 때 \n",
    "print(dir(sklearn.linear_model))    # 패키지 내부의 객체가 생각나지 않을 때 \n",
    "print(help(LogisticRegression)) # 객체 사용법이 생각나지 않을 때 - 하단의 Examples\n",
    "\n",
    "# pandas의 Series 객체의 메서드 사용법이 생각나지 않을 때 \n",
    "print(help(pd.Series.replace))\n",
    "\n",
    "print(type(pd.DataFrame.shape)) # property라고 표시되면 값! -> help 없음 \n",
    "print(type(pd.Series.replace)) # function이라고 표시되면 함수! -> help 있음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples 까지 모두 출력하는 방법 \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
